{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffae5c16",
   "metadata": {},
   "source": [
    "# Lab Manual: Practical Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d04d2cd",
   "metadata": {},
   "source": [
    "This lab will guide you through the fundamentals of word embeddings and their practical application in building a semantic search system.\n",
    "\n",
    "Unlike traditional keyword search, semantic search understands the meaning and context behind your query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9cada3",
   "metadata": {},
   "source": [
    "## Lab Objective:\n",
    "### Build a `semantic search system` that finds documents `based on meaning` rather than keyword matching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3a981e",
   "metadata": {},
   "source": [
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc9271c",
   "metadata": {},
   "source": [
    "## Part 1: Setting Up and Exploring the Building Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5976b85",
   "metadata": {},
   "source": [
    "`Concept:` A word embedding is a numerical vector representation of a word. The magic is that these vectors capture semantic relationships. \n",
    "<br>For example, the vector for `king` - `man` + `woman` results in a vector very close to that of `queen`. \n",
    "<br>We'll use a pre-trained model called `GloVe (Global Vectors for Word Representation)`, which was trained on a massive dataset from `Wikipedia`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0520332",
   "metadata": {},
   "source": [
    "`Gensim` is an open-source Python library designed for topic modeling and document similarity analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc83bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install gensim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a34b11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the gensim downloader API to fetch pre-trained models\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c7c53b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "fasttext-wiki-news-subwords-300\n",
      "conceptnet-numberbatch-17-06-300\n",
      "word2vec-ruscorpora-300\n",
      "word2vec-google-news-300\n",
      "glove-wiki-gigaword-50\n",
      "glove-wiki-gigaword-100\n",
      "glove-wiki-gigaword-200\n",
      "glove-wiki-gigaword-300\n",
      "glove-twitter-25\n",
      "glove-twitter-50\n",
      "glove-twitter-100\n",
      "glove-twitter-200\n",
      "__testing_word2vec-matrix-synopsis\n"
     ]
    }
   ],
   "source": [
    "# Let's see what models are available. There are many, trained on different data!\n",
    "available_models = list(api.info()['models'].keys())\n",
    "\n",
    "print(\"Available models:\")\n",
    "for model in available_models:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e794c50",
   "metadata": {},
   "source": [
    "We'll load `glove-wiki-gigaword-300`. <br>\n",
    "\n",
    "This is a great choice because it's powerful but small enough to run quickly. <br>\n",
    "\n",
    "The `300` means each word is represented by a `300-dimensional vector` <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c489746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache cleared!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Clear corrupted cache\n",
    "cache_dir = os.path.join(os.path.expanduser(\"~\"), \"gensim-data\")\n",
    "if os.path.exists(cache_dir):\n",
    "    shutil.rmtree(cache_dir)\n",
    "    print(\"Cache cleared!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d8fd23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "word_vectors = api.load(\"glove-wiki-gigaword-300\")  # Smaller, faster model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adc0498",
   "metadata": {},
   "source": [
    "### Basic exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f32e887c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 400000\n",
      "Vector dimension: 300\n"
     ]
    }
   ],
   "source": [
    "# Let's explore some basic properties of the model\n",
    "\n",
    "print(\"Vocabulary size:\", len(word_vectors.key_to_index)) # How many unique words it knows\n",
    "print(\"Vector dimension:\", word_vectors.vector_size) # The size of the vector for each word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ce7ac4",
   "metadata": {},
   "source": [
    "### Exploring Word Similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62e1266",
   "metadata": {},
   "source": [
    "We can use the model to find words with similar meanings. \n",
    "<br>The `most_similar` function finds the words whose vectors are closest to our target word's vector in the 300-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68931095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'uganda':\n",
      "  tanzania: 0.776\n",
      "  ugandan: 0.753\n",
      "  kenya: 0.747\n",
      "  zambia: 0.735\n",
      "  rwanda: 0.717\n",
      "Words similar to 'education':\n",
      "  educational: 0.737\n",
      "  schools: 0.679\n",
      "  teaching: 0.646\n",
      "  health: 0.640\n",
      "  curriculum: 0.640\n",
      "Words similar to 'innovation':\n",
      "  innovations: 0.701\n",
      "  technological: 0.691\n",
      "  creativity: 0.684\n",
      "  entrepreneurship: 0.655\n",
      "  innovative: 0.631\n"
     ]
    }
   ],
   "source": [
    "def explore_similarity(word):\n",
    "    \"\"\"A helper function to print the most similar words to a given word.\"\"\"\n",
    "    try:\n",
    "        similar = word_vectors.most_similar(word, topn=5)\n",
    "        print(f\"Words similar to '{word}':\")\n",
    "        # The score is the cosine similarity, ranging from -1 (opposite) to 1 (identical)\n",
    "        for w, score in similar:\n",
    "            print(f\"  {w}: {score:.3f}\")\n",
    "    except KeyError:\n",
    "        print(f\"'{word}' is not in the vocabulary.\")\n",
    "\n",
    "explore_similarity(\"uganda\")\n",
    "explore_similarity(\"education\")\n",
    "explore_similarity(\"innovation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fcc816",
   "metadata": {},
   "source": [
    "## Part 2: Document Similarity using Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f68082a",
   "metadata": {},
   "source": [
    "We have vectors for words, but how do we represent a whole document? The simplest, yet surprisingly effective, method is \n",
    "- to average the vectors of all the words in the document. This creates a single vector representing the document's `semantic center of gravity.`\n",
    "\n",
    "To compare how similar our query document is to our other documents, we'll use `Cosine Similarity`. \n",
    "\n",
    "- This measures the cosine of the angle between two vectors. \n",
    "- A value of 1 means they point in the exact same direction (very similar), \n",
    "- 0 means they are orthogonal (unrelated), and \n",
    "- -1 means they are opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8a38bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re # Import the regular expressions library for better text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e112b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_to_vector(doc, word_vectors):\n",
    "    \"\"\"\n",
    "    Converts a document to an average vector of its words.\n",
    "    This version includes basic text cleaning.\n",
    "    \"\"\"\n",
    "    # 1. Clean the text: remove punctuation and convert to lowercase\n",
    "    doc = re.sub(r'[^\\w\\s]', '', doc).lower()\n",
    "    \n",
    "    # 2. Split into words (tokens)\n",
    "    words = doc.split()\n",
    "    \n",
    "    # 3. Look up the vector for each word in our model's vocabulary\n",
    "    vectors = [word_vectors[word] for word in words if word in word_vectors]\n",
    "    \n",
    "    # 4. If no words are found in the vocabulary, return a zero vector\n",
    "    if not vectors:\n",
    "        return np.zeros(word_vectors.vector_size)\n",
    "    \n",
    "    # 5. Average the vectors to get a single representative vector for the document\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0ed2177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our sample \"database\" of documents about East Africa\n",
    "documents = [\n",
    "    \"Uganda is working to improve university education and research facilities\",\n",
    "    \"Kenyan universities are expanding their artificial intelligence programs\",\n",
    "    \"Agricultural innovation is key to economic growth in East Africa\",\n",
    "    \"The government is investing in digital infrastructure across the region\",\n",
    "    \"Student enrollment in computer science has doubled in recent years\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e138fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all our documents into numerical vectors\n",
    "doc_vectors = [document_to_vector(doc, word_vectors) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6936e0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document similarities to the query:\n",
      "1. Score: 0.840 - Uganda is working to improve university education and research facilities\n",
      "2. Score: 0.775 - Kenyan universities are expanding their artificial intelligence programs\n",
      "3. Score: 0.771 - Agricultural innovation is key to economic growth in East Africa\n",
      "4. Score: 0.748 - The government is investing in digital infrastructure across the region\n",
      "5. Score: 0.784 - Student enrollment in computer science has doubled in recent years\n"
     ]
    }
   ],
   "source": [
    "# Our search query\n",
    "query = \"higher education technology in African universities\"\n",
    "query_vector = document_to_vector(query, word_vectors)\n",
    "\n",
    "# Calculate the cosine similarity between our query vector and all document vectors\n",
    "# The result is a 2D array, so we grab the first (and only) row with [0]\n",
    "similarities = cosine_similarity([query_vector], doc_vectors)[0]\n",
    "\n",
    "print(\"\\nDocument similarities to the query:\")\n",
    "for i, (doc, sim) in enumerate(zip(documents, similarities)):\n",
    "    print(f\"{i+1}. Score: {sim:.3f} - {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381b30a9",
   "metadata": {},
   "source": [
    "Observation: Notice how the top results are about universities and education, even though they don't share many exact keywords with the query. That's semantic search in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36f7210",
   "metadata": {},
   "source": [
    "## Part 3: Building a Semantic Search System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b0189ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSearchEngine:\n",
    "    \"\"\"A simple semantic search engine.\"\"\"\n",
    "    def __init__(self, word_vectors):\n",
    "        self.word_vectors = word_vectors\n",
    "        self.documents = []\n",
    "        self.doc_vectors = []\n",
    "    \n",
    "    def add_document(self, document):\n",
    "        \"\"\"Adds a document to the engine's database.\"\"\"\n",
    "        self.documents.append(document)\n",
    "        # We pre-calculate and store the vector to make searching faster\n",
    "        self.doc_vectors.append(document_to_vector(document, self.word_vectors))\n",
    "    \n",
    "    def search(self, query, top_k=3):\n",
    "        \"\"\"Searches the documents for the most relevant results to the query.\"\"\"\n",
    "        query_vector = document_to_vector(query, self.word_vectors)\n",
    "        \n",
    "        # Calculate similarity against all stored document vectors\n",
    "        similarities = cosine_similarity([query_vector], self.doc_vectors)[0]\n",
    "        \n",
    "        # Combine documents with their scores and sort to find the best matches\n",
    "        results = sorted(zip(self.documents, similarities), \n",
    "                         key=lambda item: item[1],  # Sort by the score\n",
    "                         reverse=True)             # Highest score first\n",
    "        \n",
    "        return results[:top_k] # Return the top_k results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fbfd7a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Query: 'university programs' ---\n",
      "  Score: 0.755 - Uganda is working to improve university education and research facilities\n",
      "  Score: 0.686 - Student enrollment in computer science has doubled in recent years\n",
      "  Score: 0.664 - Kenyan universities are expanding their artificial intelligence programs\n",
      "\n",
      "--- Query: 'technology investment' ---\n",
      "  Score: 0.645 - Agricultural innovation is key to economic growth in East Africa\n",
      "  Score: 0.635 - The government is investing in digital infrastructure across the region\n",
      "  Score: 0.610 - Uganda is working to improve university education and research facilities\n",
      "\n",
      "--- Query: 'agricultural development' ---\n",
      "  Score: 0.741 - Agricultural innovation is key to economic growth in East Africa\n",
      "  Score: 0.647 - Uganda is working to improve university education and research facilities\n",
      "  Score: 0.583 - The government is investing in digital infrastructure across the region\n"
     ]
    }
   ],
   "source": [
    "# 1. Create an instance of our search engine\n",
    "search_engine = SemanticSearchEngine(word_vectors)\n",
    "\n",
    "# 2. Add our documents to the engine\n",
    "for doc in documents:\n",
    "    search_engine.add_document(doc)\n",
    "\n",
    "# 3. Let's test it with a few queries!\n",
    "test_queries = [\n",
    "    \"university programs\",\n",
    "    \"technology investment\",\n",
    "    \"agricultural development\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n--- Query: '{query}' ---\")\n",
    "    results = search_engine.search(query)\n",
    "    for doc, score in results:\n",
    "        print(f\"  Score: {score:.3f} - {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6845a212",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bca5c9",
   "metadata": {},
   "source": [
    "## Part 4: Visualization of Word Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399c7509",
   "metadata": {},
   "source": [
    "Our word vectors have `300` dimensions, which we can't possibly visualize. \n",
    "<br>We can use a dimensionality reduction technique called `t-SNE (t-Distributed Stochastic Neighbor Embedding)` to project these vectors down to 2D space. \n",
    "- This helps us visually confirm that words with similar meanings cluster together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39094534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "921bc461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_relationships(words, word_vectors, title):\n",
    "    \"\"\"Plots a list of words in 2D space using t-SNE.\"\"\"\n",
    "    # Get the 300-dimensional vectors for our chosen words\n",
    "    vectors = [word_vectors[word] for word in words if word in word_vectors]\n",
    "    words_filtered = [word for word in words if word in word_vectors]\n",
    "    \n",
    "    # Use t-SNE to reduce the 300 dimensions down to 2\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=len(words_filtered)-2)\n",
    "    vectors_2d = tsne.fit_transform(vectors)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], color='steelblue')\n",
    "    \n",
    "    # Annotate each point with its corresponding word\n",
    "    for i, word in enumerate(words_filtered):\n",
    "        plt.annotate(word, xy=(vectors_2d[i, 0], vectors_2d[i, 1]), xytext=(5, 2),\n",
    "                     textcoords='offset points', ha='right', va='bottom')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"t-SNE Dimension 1\")\n",
    "    plt.ylabel(\"t-SNE Dimension 2\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1beb553f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# A list of words related to technology\u001b[39;00m\n\u001b[0;32m      2\u001b[0m tech_words \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomputer\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftware\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minternet\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malgorithm\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      3\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprogramming\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnetwork\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdigital\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatabase\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcloud\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m \u001b[43mplot_word_relationships\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtech_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTechnology Words Semantic Space\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# A list of words related to education\u001b[39;00m\n\u001b[0;32m      7\u001b[0m edu_words \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muniversity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meducation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstudent\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mteacher\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassroom\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresearch\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mknowledge\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mschool\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprofessor\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdegree\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[32], line 9\u001b[0m, in \u001b[0;36mplot_word_relationships\u001b[1;34m(words, word_vectors, title)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Use t-SNE to reduce the 300 dimensions down to 2\u001b[39;00m\n\u001b[0;32m      8\u001b[0m tsne \u001b[38;5;241m=\u001b[39m TSNE(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, perplexity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(words_filtered)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m vectors_2d \u001b[38;5;241m=\u001b[39m \u001b[43mtsne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Create the plot\u001b[39;00m\n\u001b[0;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\HAMU COMPUTERS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\HAMU COMPUTERS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HAMU COMPUTERS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1143\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(\n\u001b[0;32m   1119\u001b[0m     \u001b[38;5;66;03m# TSNE.metric is not validated yet\u001b[39;00m\n\u001b[0;32m   1120\u001b[0m     prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1121\u001b[0m )\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit X into an embedded space and return that transformed output.\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \n\u001b[0;32m   1125\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;124;03m        Embedding of the training data in low-dimensional space.\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1143\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_params_vs_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1144\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X)\n\u001b[0;32m   1145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ \u001b[38;5;241m=\u001b[39m embedding\n",
      "File \u001b[1;32mc:\\Users\\HAMU COMPUTERS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:846\u001b[0m, in \u001b[0;36mTSNE._check_params_vs_input\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    845\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_params_vs_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m--> 846\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperplexity \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m    847\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    848\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperplexity (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperplexity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be less \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    849\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthan n_samples (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    850\u001b[0m         )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# A list of words related to technology\n",
    "tech_words = ['computer', 'software', 'internet', 'algorithm', 'data', \n",
    "              'programming', 'network', 'digital', 'code', 'database', 'cloud']\n",
    "plot_word_relationships(tech_words, word_vectors, \"Technology Words Semantic Space\")\n",
    "\n",
    "# A list of words related to education\n",
    "edu_words = ['university', 'education', 'student', 'teacher', 'classroom',\n",
    "             'learning', 'research', 'knowledge', 'school', 'professor', 'degree']\n",
    "plot_word_relationships(edu_words, word_vectors, \"Education Words Semantic Space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce4487b",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b2a073",
   "metadata": {},
   "source": [
    "## Part 5: Discussion & Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fb9e9a",
   "metadata": {},
   "source": [
    "This method is powerful, but it's important to understand its limitations:\n",
    "\n",
    "- Loss of Word Order: Averaging vectors treats a document as a \"bag of words.\" The meaning from word order is lost. \"Dog bites man\" and \"Man bites dog\" would have the exact same document vector.\n",
    "\n",
    "- Negation is Tricky: The model doesn't handle negation well. \"A good day\" and \"not a good day\" might have very similar vectors.\n",
    "\n",
    "- Out-of-Vocabulary (OOV) Words: The model only knows words it was trained on. New slang, technical jargon, or misspellings will be ignored.\n",
    "\n",
    "- Averaging Ambiguity: If a document contains words with opposite meanings (e.g., \"happy\" and \"sad\"), the average vector might be a muddled, meaningless representation in the middle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bd4608",
   "metadata": {},
   "source": [
    "#### Practice Scenario & Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9432a3",
   "metadata": {},
   "source": [
    "#### You are working for a news organization. Your task is to build a tool that helps journalists find articles related to a specific topic, even if they don't use the exact same keywords.\n",
    "\n",
    "Dataset <br>\n",
    "You will use the famous `20 Newsgroups` dataset, which is conveniently built into scikit-learn. It contains about 18,000 newsgroup posts on 20 different topics.\n",
    "\n",
    "Guide Questions\n",
    "1. Load the Data: Load the data for just two categories: rec.sport.hockey and talk.politics.guns.\n",
    "\n",
    "2. Build the Search Engine: Create a new SemanticSearchEngine instance and add all the documents from both categories to it.\n",
    "\n",
    "3. Test Your Engine: Run searches with the following queries and analyze the results. Which category do the top results come from? Do the results make sense?\n",
    "\n",
    "    - Query 1: protecting constitutional rights\n",
    "\n",
    "    - Query 2: the playoff season\n",
    "\n",
    "    - Query 3 (tricky one!): violent confrontation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
