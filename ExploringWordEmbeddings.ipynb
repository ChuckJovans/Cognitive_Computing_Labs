{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffae5c16",
   "metadata": {},
   "source": [
    "# Lab Manual: Practical Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d04d2cd",
   "metadata": {},
   "source": [
    "This lab will guide you through the fundamentals of word embeddings and their practical application in building a semantic search system.\n",
    "\n",
    "Unlike traditional keyword search, semantic search understands the meaning and context behind your query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9cada3",
   "metadata": {},
   "source": [
    "## Lab Objective:\n",
    "### Build a `semantic search system` that finds documents `based on meaning` rather than keyword matching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3a981e",
   "metadata": {},
   "source": [
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc9271c",
   "metadata": {},
   "source": [
    "## Part 1: Setting Up and Exploring the Building Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5976b85",
   "metadata": {},
   "source": [
    "`Concept:` A word embedding is a numerical vector representation of a word. The magic is that these vectors capture semantic relationships. \n",
    "<br>For example, the vector for `king` - `man` + `woman` results in a vector very close to that of `queen`. \n",
    "<br>We'll use a pre-trained model called `GloVe (Global Vectors for Word Representation)`, which was trained on a massive dataset from `Wikipedia`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0520332",
   "metadata": {},
   "source": [
    "`Gensim` is an open-source Python library designed for topic modeling and document similarity analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc83bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install gensim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a34b11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the gensim downloader API to fetch pre-trained models\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c7c53b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "fasttext-wiki-news-subwords-300\n",
      "conceptnet-numberbatch-17-06-300\n",
      "word2vec-ruscorpora-300\n",
      "word2vec-google-news-300\n",
      "glove-wiki-gigaword-50\n",
      "glove-wiki-gigaword-100\n",
      "glove-wiki-gigaword-200\n",
      "glove-wiki-gigaword-300\n",
      "glove-twitter-25\n",
      "glove-twitter-50\n",
      "glove-twitter-100\n",
      "glove-twitter-200\n",
      "__testing_word2vec-matrix-synopsis\n"
     ]
    }
   ],
   "source": [
    "# Let's see what models are available. There are many, trained on different data!\n",
    "available_models = list(api.info()['models'].keys())\n",
    "\n",
    "print(\"Available models:\")\n",
    "for model in available_models:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e794c50",
   "metadata": {},
   "source": [
    "We'll load `glove-wiki-gigaword-300`. <br>\n",
    "\n",
    "This is a great choice because it's powerful but small enough to run quickly. <br>\n",
    "\n",
    "The `300` means each word is represented by a `300-dimensional vector` <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c489746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache cleared!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Clear corrupted cache\n",
    "cache_dir = os.path.join(os.path.expanduser(\"~\"), \"gensim-data\")\n",
    "if os.path.exists(cache_dir):\n",
    "    shutil.rmtree(cache_dir)\n",
    "    print(\"Cache cleared!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d8fd23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "word_vectors = api.load(\"glove-wiki-gigaword-300\")  # Smaller, faster model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adc0498",
   "metadata": {},
   "source": [
    "### Basic exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32e887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore some basic properties of the model\n",
    "\n",
    "print(\"Vocabulary size:\", len(word_vectors.key_to_index)) # How many unique words it knows\n",
    "print(\"Vector dimension:\", word_vectors.vector_size) # The size of the vector for each word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ce7ac4",
   "metadata": {},
   "source": [
    "### Exploring Word Similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62e1266",
   "metadata": {},
   "source": [
    "We can use the model to find words with similar meanings. \n",
    "<br>The `most_similar` function finds the words whose vectors are closest to our target word's vector in the 300-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68931095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_similarity(word):\n",
    "    \"\"\"A helper function to print the most similar words to a given word.\"\"\"\n",
    "    try:\n",
    "        similar = word_vectors.most_similar(word, topn=5)\n",
    "        print(f\"Words similar to '{word}':\")\n",
    "        # The score is the cosine similarity, ranging from -1 (opposite) to 1 (identical)\n",
    "        for w, score in similar:\n",
    "            print(f\"  {w}: {score:.3f}\")\n",
    "    except KeyError:\n",
    "        print(f\"'{word}' is not in the vocabulary.\")\n",
    "\n",
    "explore_similarity(\"uganda\")\n",
    "explore_similarity(\"education\")\n",
    "explore_similarity(\"innovation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fcc816",
   "metadata": {},
   "source": [
    "## Part 2: Document Similarity using Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f68082a",
   "metadata": {},
   "source": [
    "We have vectors for words, but how do we represent a whole document? The simplest, yet surprisingly effective, method is \n",
    "- to average the vectors of all the words in the document. This creates a single vector representing the document's `semantic center of gravity.`\n",
    "\n",
    "To compare how similar our query document is to our other documents, we'll use `Cosine Similarity`. \n",
    "\n",
    "- This measures the cosine of the angle between two vectors. \n",
    "- A value of 1 means they point in the exact same direction (very similar), \n",
    "- 0 means they are orthogonal (unrelated), and \n",
    "- -1 means they are opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a38bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re # Import the regular expressions library for better text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e112b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_to_vector(doc, word_vectors):\n",
    "    \"\"\"\n",
    "    Converts a document to an average vector of its words.\n",
    "    This version includes basic text cleaning.\n",
    "    \"\"\"\n",
    "    # 1. Clean the text: remove punctuation and convert to lowercase\n",
    "    doc = re.sub(r'[^\\w\\s]', '', doc).lower()\n",
    "    \n",
    "    # 2. Split into words (tokens)\n",
    "    words = doc.split()\n",
    "    \n",
    "    # 3. Look up the vector for each word in our model's vocabulary\n",
    "    vectors = [word_vectors[word] for word in words if word in word_vectors]\n",
    "    \n",
    "    # 4. If no words are found in the vocabulary, return a zero vector\n",
    "    if not vectors:\n",
    "        return np.zeros(word_vectors.vector_size)\n",
    "    \n",
    "    # 5. Average the vectors to get a single representative vector for the document\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ed2177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our sample \"database\" of documents about East Africa\n",
    "documents = [\n",
    "    \"Uganda is working to improve university education and research facilities\",\n",
    "    \"Kenyan universities are expanding their artificial intelligence programs\",\n",
    "    \"Agricultural innovation is key to economic growth in East Africa\",\n",
    "    \"The government is investing in digital infrastructure across the region\",\n",
    "    \"Student enrollment in computer science has doubled in recent years\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e138fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all our documents into numerical vectors\n",
    "doc_vectors = [document_to_vector(doc, word_vectors) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6936e0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our search query\n",
    "query = \"higher education technology in African universities\"\n",
    "query_vector = document_to_vector(query, word_vectors)\n",
    "\n",
    "# Calculate the cosine similarity between our query vector and all document vectors\n",
    "# The result is a 2D array, so we grab the first (and only) row with [0]\n",
    "similarities = cosine_similarity([query_vector], doc_vectors)[0]\n",
    "\n",
    "print(\"\\nDocument similarities to the query:\")\n",
    "for i, (doc, sim) in enumerate(zip(documents, similarities)):\n",
    "    print(f\"{i+1}. Score: {sim:.3f} - {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381b30a9",
   "metadata": {},
   "source": [
    "Observation: Notice how the top results are about universities and education, even though they don't share many exact keywords with the query. That's semantic search in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36f7210",
   "metadata": {},
   "source": [
    "## Part 3: Building a Semantic Search System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0189ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSearchEngine:\n",
    "    \"\"\"A simple semantic search engine.\"\"\"\n",
    "    def __init__(self, word_vectors):\n",
    "        self.word_vectors = word_vectors\n",
    "        self.documents = []\n",
    "        self.doc_vectors = []\n",
    "    \n",
    "    def add_document(self, document):\n",
    "        \"\"\"Adds a document to the engine's database.\"\"\"\n",
    "        self.documents.append(document)\n",
    "        # We pre-calculate and store the vector to make searching faster\n",
    "        self.doc_vectors.append(document_to_vector(document, self.word_vectors))\n",
    "    \n",
    "    def search(self, query, top_k=3):\n",
    "        \"\"\"Searches the documents for the most relevant results to the query.\"\"\"\n",
    "        query_vector = document_to_vector(query, self.word_vectors)\n",
    "        \n",
    "        # Calculate similarity against all stored document vectors\n",
    "        similarities = cosine_similarity([query_vector], self.doc_vectors)[0]\n",
    "        \n",
    "        # Combine documents with their scores and sort to find the best matches\n",
    "        results = sorted(zip(self.documents, similarities), \n",
    "                         key=lambda item: item[1],  # Sort by the score\n",
    "                         reverse=True)             # Highest score first\n",
    "        \n",
    "        return results[:top_k] # Return the top_k results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfd7a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create an instance of our search engine\n",
    "search_engine = SemanticSearchEngine(word_vectors)\n",
    "\n",
    "# 2. Add our documents to the engine\n",
    "for doc in documents:\n",
    "    search_engine.add_document(doc)\n",
    "\n",
    "# 3. Let's test it with a few queries!\n",
    "test_queries = [\n",
    "    \"university programs\",\n",
    "    \"technology investment\",\n",
    "    \"agricultural development\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n--- Query: '{query}' ---\")\n",
    "    results = search_engine.search(query)\n",
    "    for doc, score in results:\n",
    "        print(f\"  Score: {score:.3f} - {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6845a212",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bca5c9",
   "metadata": {},
   "source": [
    "## Part 4: Visualization of Word Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399c7509",
   "metadata": {},
   "source": [
    "Our word vectors have `300` dimensions, which we can't possibly visualize. \n",
    "<br>We can use a dimensionality reduction technique called `t-SNE (t-Distributed Stochastic Neighbor Embedding)` to project these vectors down to 2D space. \n",
    "- This helps us visually confirm that words with similar meanings cluster together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39094534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921bc461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_relationships(words, word_vectors, title):\n",
    "    \"\"\"Plots a list of words in 2D space using t-SNE.\"\"\"\n",
    "    # Get the 300-dimensional vectors for our chosen words\n",
    "    vectors = [word_vectors[word] for word in words if word in word_vectors]\n",
    "    words_filtered = [word for word in words if word in word_vectors]\n",
    "    \n",
    "    # Use t-SNE to reduce the 300 dimensions down to 2\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=len(words_filtered)-2)\n",
    "    vectors_2d = tsne.fit_transform(vectors)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], color='steelblue')\n",
    "    \n",
    "    # Annotate each point with its corresponding word\n",
    "    for i, word in enumerate(words_filtered):\n",
    "        plt.annotate(word, xy=(vectors_2d[i, 0], vectors_2d[i, 1]), xytext=(5, 2),\n",
    "                     textcoords='offset points', ha='right', va='bottom')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"t-SNE Dimension 1\")\n",
    "    plt.ylabel(\"t-SNE Dimension 2\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beb553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of words related to technology\n",
    "tech_words = ['computer', 'software', 'internet', 'algorithm', 'data', \n",
    "              'programming', 'network', 'digital', 'code', 'database', 'cloud']\n",
    "plot_word_relationships(tech_words, word_vectors, \"Technology Words Semantic Space\")\n",
    "\n",
    "# A list of words related to education\n",
    "edu_words = ['university', 'education', 'student', 'teacher', 'classroom',\n",
    "             'learning', 'research', 'knowledge', 'school', 'professor', 'degree']\n",
    "plot_word_relationships(edu_words, word_vectors, \"Education Words Semantic Space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce4487b",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b2a073",
   "metadata": {},
   "source": [
    "## Part 5: Discussion & Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fb9e9a",
   "metadata": {},
   "source": [
    "This method is powerful, but it's important to understand its limitations:\n",
    "\n",
    "- Loss of Word Order: Averaging vectors treats a document as a \"bag of words.\" The meaning from word order is lost. \"Dog bites man\" and \"Man bites dog\" would have the exact same document vector.\n",
    "\n",
    "- Negation is Tricky: The model doesn't handle negation well. \"A good day\" and \"not a good day\" might have very similar vectors.\n",
    "\n",
    "- Out-of-Vocabulary (OOV) Words: The model only knows words it was trained on. New slang, technical jargon, or misspellings will be ignored.\n",
    "\n",
    "- Averaging Ambiguity: If a document contains words with opposite meanings (e.g., \"happy\" and \"sad\"), the average vector might be a muddled, meaningless representation in the middle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bd4608",
   "metadata": {},
   "source": [
    "#### Practice Scenario & Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9432a3",
   "metadata": {},
   "source": [
    "#### You are working for a news organization. Your task is to build a tool that helps journalists find articles related to a specific topic, even if they don't use the exact same keywords.\n",
    "\n",
    "Dataset <br>\n",
    "You will use the famous `20 Newsgroups` dataset, which is conveniently built into scikit-learn. It contains about 18,000 newsgroup posts on 20 different topics.\n",
    "\n",
    "Guide Questions\n",
    "1. Load the Data: Load the data for just two categories: rec.sport.hockey and talk.politics.guns.\n",
    "\n",
    "2. Build the Search Engine: Create a new SemanticSearchEngine instance and add all the documents from both categories to it.\n",
    "\n",
    "3. Test Your Engine: Run searches with the following queries and analyze the results. Which category do the top results come from? Do the results make sense?\n",
    "\n",
    "    - Query 1: protecting constitutional rights\n",
    "\n",
    "    - Query 2: the playoff season\n",
    "\n",
    "    - Query 3 (tricky one!): violent confrontation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
